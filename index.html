<html><head><meta http-equiv="Content-Type" content="text/html; charset=GBK">
<title>Lin Shao</title>
<link rel="stylesheet" type="text/css" href="./files/main.css">
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CLMERFHZCQ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CLMERFHZCQ');
</script>
</head>


<body>

<table>
<tbody><tr>
<td><img src="./files/lins2.png" width="200"></td>
<td>
<div style="font-size:24; font-weight:bold">Lin Shao</div>
<div>
Assistant Professor at<br>
<a href="https://www.comp.nus.edu.sg/cs/"> Department of Computer Science</a>,<a href="https://www.comp.nus.edu.sg/"> School of Computing</a><br>
<a href="https://www.nus.edu.sg/">National University of Singapore</a><br>
</div>
<div>
<b>Email:</b> <tt>linshao at nus dot edu dot sg</tt><br>
<b>Office:</b>COM2-03-03<br>
</div>
<div>
<a href="https://linsats.github.io/#publications">[Publications]</a>&nbsp;
<a href="">[Experiences]</a>&nbsp;
<a href="https://linsats.github.io/#teaching">[Teaching]</a>&nbsp;
<a href="">[Misc]</a>&nbsp;
<br><br>
<a href="https://scholar.google.com/citations?user=UU76Pg4AAAAJ&hl=en">[Google Scholar]</a>
<a href="https://github.com/linsats">[GitHub]</a>
</div>
</td>
</tr>
</tbody></table>

<script type="text/javascript">
function hideshow(which){
if (!document.getElementById)
return
if (which.style.display=="block")
which.style.display="none"
else
which.style.display="block"
}
</script>


<h3>About</h3>
<div class="section">
<ul>
My research interests lie at the intersection of robotics and artificial intelligence. My long-term goal is to holistically build general-purpose robotic systems that intelligently perform a diverse range of tasks in a large variety of environments in the physical world. Specifically, my group is interested in developing algorithms and systems to provide robots with the abilities of perception and manipulation. I serve as a co-chair of the Technical Committee on Robot Learning in the IEEE Robotics and Automation Society and an Associate Editor of IEEE Robotics and Automation Letters.<br><br>

I received my Ph.D. from Stanford University, advised by Professor <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a> and co-advised by Professor <a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a>. Prior to joining Stanford,  I was an undergraduate at <a href="https://www.nju.edu.cn/EN//">Nanjing University</a>.</ul>
</div>

<h4>PhD Dissertation Committee</h4>
<div class="subsubsection">
<ul>
<a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a> (Advisor)<br>
<a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a> (Co-advisor)<br>
<a href="https://khatib.stanford.edu/">Oussama Khatib</a>  <br>
<a href="https://sr.stanford.edu/?page_id=1217">J. Kenneth Salisbury</a>  <br>
<a href="http://bdml.stanford.edu/Profiles/MarkCutkosky">Mark Cutkosky</a> (Chair) <br>
<ul>
</div>

<h3>News</h3>
<div class="section"> 
<li> <b style="color: red; ">NEW</b> [Nov, 2024] D(R,O) Grasp won Best Robotics Paper Award at  <a href="https://sites.google.com/view/corl-mapodel-workshop/schedule">CoRL workshop</a></b>  
<li> <b style="color: red; ">NEW</b> [Nov, 2024] Invited Talk at <a href="https://svl.stanford.edu/">Stanford Vision and Learning Lab</a> </b>  
<li> <b style="color: red; ">NEW</b> [Sept, 2024] Invited Talk at CoRL 2024 Workshop on Differentiable Optimization Everywhere: Simulation, Estimation, Learning, and Control </b>  
<li> <b style="color: red; ">NEW</b> [Sept, 2024] One paper was accepted to NeurIPS 2024</b>     
<li> <b style="color: red; ">NEW</b> [Sept, 2024] TieBot was accepted to CoRL 2024 as Oral</b>    
<li> <b style="color: red; ">NEW</b> [Sept, 2024] Two papers are accepted to CoRL 2024</b>    
<li> <b style="color: red; ">NEW</b> [Aug, 2024] SAM-RL is accepted to IJRR</b>     
<li> <b style="color: red; ">NEW</b> [Aug, 2024] I serve as an Associate Editor at ICRA 2025</b>     
<li> <b style="color: red; ">NEW</b> [July, 2024] Roller Grasper V3 was accepted to TR-O </b>     
<li> <b style="color: red; ">NEW</b> [July, 2024] Two papers were accepted to IROS 2024  </b>        
<li>  [May, 2024] One paper was accepted to ICML 2024     
 <li> [May, 2024] Invited Talk at ICRA 2024 Workshop on A Future Roadmap for Sensorimotor Skill Learning for Robot Manipulation
 <li> [April, 2024] I serve as an Associate Editor of IEEE Robotics and Automation Letters
 <li> [June, 2023]  <b style="color: red; ">SAM-RL wons Best System Paper Award finalist at RSS 2023</b> 
</ul>
</div>


<h3>Joining My Group</h3>
<div class="section">
<ul>
<b style="color: green">I am looking for student researchers and postdocs to join my group at NUS. My group also has multiple positions for summer interns and visiting students. Please feel free to send me an email with your CV if you are interested in doing research with me. </b> <br>


</ul></div>    

<h3>People</h3>
<h4>PhD Students</h4>
<ul><li><a href="https://chongkaigao.com/">Chongkai Gao</a> (Tsinghua University)</li></ul>
<ul><li><a href="https://ariszxxu.github.io/">Zhixuan Xu</a> (Zhejiang University)</li></ul>
<ul><li><a href="https://crtie.github.io/">Chenrui Tie</a> (Peking University)</li></ul>
<ul><li><a href="https://panda-shawn.github.io/">Zixuan Liu</a> (Tsinghua University)</li></ul>
<ul><li><a href="https://sgtvincent.github.io">Junting Chen</a> (ETH Zurich)</li></ul>
<ul><li><a href="https://chenhn02.github.io/"> Haonan Chen</a> (Nanjing University)</li></ul>
<ul><li><a href="https://houyiwen.github.io/">Yiwen Hou</a> (University of Science and Technology of China)</li></ul>


<h3>Master's Students</h3>
<ul><li>Debang Wang (National University of Singapore)</li></ul>
<ul><li>Haoyu Zhou (Beihang University)</li></ul>
<ul><li>Hanwei Fan (Peking University)</li></ul>
<ul><li>Jiayu Luo (Beijing Institute of Technology)</li></ul>
<ul><li>Zhehao Cai (Nanjing University of Aeronautics and Astronautics)</li></ul>
<ul><li>Xiao Liu (Beihang University)</li></ul>


<h3>Visiting Scholar</h3>
<ul><li><a href="https://chenthree.github.io//">Siang Chen </a> (Tsinghua University, CSC funded)</li></ul>
<ul><li>Zhengbang Zhu (Shanghai Jiaotong University, CSC funded)</li></ul>
<ul><li>Zixuan Chen (Nanjing University, CSC funded)</li></ul>

<h3>Research Assistants</h3>
<ul><li>Hojin Bae (Peking University)</li></ul>
<ul><li><a href="https://borisguo6.github.io/">Jingxiang Guo</a> (Harbin Institute of Technology, Shenzhen)</li></ul>
<ul><li>Checheng Yu (Nanjing University)</li></ul>
<ul><li>Xunzhe Zhou (Fudan University)</li></ul>
<ul><li><a href="https://zhenyuwei2003.github.io//">Zhenyu Wei </a>(Shanghai Jiaotong University)</li></ul>
<ul><li>Jingxian Zhou (Shanghai Jiaotong University)</li></ul>
<ul><li>Mingxu Zhang (Beijing University of Posts and Telecommunications)</li></ul>
<ul><li>Haozhuo Zhang (Peking University)</li></ul>


<h3>Alumni</h3>
<ul><li>Mingxin Yu (Peking University), Next Ph.D. student at MIT</li></ul> 
<ul><li>Yifan You (UCLA), Next Ph.D. student at Berkeley</ul> 
<ul><li>Shuo Cheng (UCSD), Next Ph.D. student at Georgia Tech</li></ul>
<ul><li>Ruiqi Zhang (Tongji University), Next Ph.D. student at Berkeley</li></ul>
<ul><li>Weikun Peng (Beihang University), Next Ph.D. student at SFU</li></ul>

<a name="publications"></a>
<h3>Recent Publications</h3>
<div class="mainsection">
<ul>
<table width="100%">

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/emos.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://github.com/SgtVincent/EMOS">EMOS: Embodiment-aware Heterogeneous Multi-robot Operating System with LLM Agents</a></b><br><br> Junting Chen, Checheng Yu, Xunzhe Zhou, Tianqi Xu, Yao Mu, Mengkang Hu, Wenqi Shao, Yikai Wang, Guohao Li, Lin Shao
<br><br>Arxiv <br><br>
<a href="https://arxiv.org/abs/2410.22662">paper</a>
&nbsp;<a href="https://github.com/SgtVincent/EMOS/">project</a>
&nbsp;video
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>



<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/flip.png" width="180" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://flow-planning.github.io">FLIP: Flow-Centric Generative Planning for General-Purpose Manipulation Tasks</a></b><br><br> Chongkai Gao, Haozhuo Zhang, Zhixuan Xu, Zhehao Cai, Lin Shao
<br><br>CoRL 2024 Workshop LEAP <b style="color: red; ">(Oral)<br><br>
<a href="https://chongkaigao.com/files/FLIP.pdf">paper</a>
&nbsp;<a href="https://flow-planning.github.io/">project</a>
&nbsp;<a href="https://flow-planning.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/drograsp.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://nus-lins-lab.github.io/drograspweb/">D(R,O) Grasp: A Unified Representation of Robot and Object Interaction for Cross-Embodiment Dexterous Grasping</a></b><br><br> Zhenyu Wei*, Zhixuan Xu*, Jingxiang Guo, Yiwen Hou, Chongkai Gao, Zhehao Cai, Jiayu Luo, Lin Shao
<br><br>IEEE International Conference on Robotics and Automation (ICRA) 2025 (under review)
<br><br>      <b style="color: red; ">Best Robotics  Paper Award at CoRL MAPoDeL Workshop </b>  <br><br>
<a href="https://arxiv.org/abs/2410.01702">paper</a>
&nbsp;<a href="https://nus-lins-lab.github.io/drograspweb/">project</a>
&nbsp;<a href="https://nus-lins-lab.github.io/drograspweb/">video</a> 
&nbsp;<a href="https://github.com/zhenyuwei2003/DRO-Grasp">code</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/tiebot.png" width="140" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/tiebot">TieBot: Model-based Learning to Knot a Tie from Visual
Demonstration via Differentiable Physics-based Simulation</a></b><br><br> Weikun Peng, Jun Lv, Yuwei Zeng, Haonan Chen, Siheng Zhao, Jichen Sun, Cewu Lu, and Lin Shao
<br><br>Conference on Robot Learning (CoRL) 2024 <b style="color: red; ">(Oral)<br><br>
<a href="https://arxiv.org/pdf/2407.03245">paper</a>
&nbsp;<a href="https://tiebots.github.io/">project</a>
&nbsp;<a href="https://tiebots.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/SE3.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://riemann-web.github.io/">RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation</a></b><br><br> Chongkai Gao, Zhengrong Xue, Shuying Deng, Tianhai Liang, Siqi Yang, Lin Shao, Huazhe Xu
<br><br>Conference on Robot Learning (CoRL) 2024 <br><br>
<a href="https://arxiv.org/abs/2403.19460">paper</a>
&nbsp;<a href="https://riemann-web.github.io/">project</a>
&nbsp;<a href="https://riemann-web.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>
    

<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/rollerv3.png" height="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://yuanshenli.com/roller_grasper_v3.html">Design and Control of Roller Grasper V3 for
In-Hand Manipulation</a></b><br><br> Shenli Yuan, Lin Shao, Yunhai Feng, Jiatong Sun, Teng Xue, Connor
Yako, Jeannette Bohg, Kenneth Salisbury<br><br>
IEEE Transactions on Robotics<br><br>
<a href="https://ieeexplore.ieee.org/document/10666738">paper</a>
&nbsp;<a href="https://yuanshenli.com/roller_grasper_v3.html">project</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody> 



<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/manifm.png" height="140" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://manifoundationmodel.github.io/">ManiFoundation Model for General-Purpose Robotic Manipulation of Contact Synthesis with Arbitrary Objects and Robots</a></b><br><br> Zhixuan Xu*, Chongkai Gao*, Zixuan Liu*, Gang Yang*, Chenrui Tie, Haozhuo Zheng, Haoyu Zhou, Weikun Peng, Debang Wang, Tianyi Chen, Zhouliang Yu, Lin Shao<br><br>
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2024 <b style="color: red; ">(Oral)<br><br>
<a href="https://arxiv.org/pdf/2405.06964">paper</a>
&nbsp;<a href="https://manifoundationmodel.github.io/">project</a>
&nbsp;<a href="https://manifoundationmodel.github.io/">video</a> 
&nbsp;<a href="https://github.com/NUS-LinS-Lab/ManiFM">code</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/mpm.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://linsats.github.io">SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact
Model and Two-way Coupling with Articulated Rigid Bodies and Clothes</a></b><br><br> Min Liu, Gang Yang, Siyuan Luo, Chen Yu, and Lin Shao<br><br>
  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2024 <br><br>
<a href="https://linsats.github.io">paper</a>
&nbsp;<a href="https://sites.google.com/view/softmac">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/learning2rank.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/rewardselfalign">Learning Reward for Robot Skills Using Large Language Models via
Self-Alignment</a></b><br><br>Yuwei Zeng, Yao Mu, Lin Shao <br><br>
 International Conference on Machine Learning (ICML) 2024<br><br>
<a href="https://arxiv.org/pdf/2405.07162">paper</a>
&nbsp;<a href="https://sites.google.com/view/rewardselfalign/">project</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/llm.png" width="145" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="ttps://object814.github.io/Task-Condition-With-LLM/">Generalizable Long-Horizon Manipulations
with Large Language Models</a></b><br><br> Haoyu Zhou, Mingyu Ding, Weikun Peng, Masayoshi Tomizuka, Lin Shao and Chuang Gan<br><br>
Arxiv <br><br>
<a href="https://arxiv.org/pdf/2310.02264v1.pdf">paper</a>
&nbsp;<a href="https://object814.github.io/Task-Condition-With-LLM/">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>








<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/m3dpart.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/cloth-grasping">Category-Level Multi-Part Multi-Joint 3D Shape Assembly</a></b><br><br>Yichen Li, Kaichun Mo, Yueqi Duan, He Wang, Jiequan Zhang, Lin Shao, Wojciech Matusik and Leonidas J. Guibas<br><br>
<a href="https://linsats.github.io">IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024 (CVPR)<br><br>
&nbsp;<a href="https://sites.google.com/view/cloth-grasping">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/adamas.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/diffsim/">Jade: A Differentiable Physics Engine for Articulated Rigid Bodies with Intersection-Free Frictional Contact</a></b><br><br> Gang Yang, Siyuan Luo, Yunhai Feng, Zhixin Sun, Chenrui Tie, and Lin Shao<br><br>
    IEEE International Conference on Robotics and Automation (ICRA) 2024 <br><br>
<a href="https://arxiv.org/abs/2309.04710">paper</a>
&nbsp;<a href="https://sites.google.com/view/diffsim/">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/gamma.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://linsats.github.io">GAMMA: Generalizable Articulation Modeling and Manipulation for Articulated Objects</a></b><br><br> Qiaojun Yu, Junbo Wang, Wenhai Liu, Ce Hao, Liu Liu, Lin Shao, Weiming Wang and Cewu Lu<br><br>
    IEEE International Conference on Robotics and Automation (ICRA) 2024  <br><br>
<a href="https://linsats.github.io">paper</a>
&nbsp;<a href="https://linsats.github.io/">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>



<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/difflfd.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/diff-lfd/">Diff-LfD: Contact-aware Model-based Learning from Visual
Demonstration for Robotic Manipulation via Differentiable Physics-based
Simulation and Rendering</a></b><br><br> Xinghao Zhu, Jinghan Ke, Zhixuan Xu, Zhixin Sun, Bizhe Bai, Jun Lv, Qingtao Liu, Yuwei Zeng, Qi Ye, Cewu Lu, Masayoshi Tomizuka, Lin Shao<br><br>Conference on Robot Learning (CoRL) 2023  <b style="color: red; ">(Oral)<br><br>
<a href="https://linsats.github.io/">paper</a>
&nbsp;<a href="https://sites.google.com/view/diff-lfd">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/clothesnet.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/clothesnet/">ClothesNet: An Information-Rich 3D Garment Model Repository with Simulated Clothes Environment</a></b><br><br> Bingyang Zhou, Haoyu Zhou, Tianhai Liang, Qiaojun Yu, Siheng Zhao, Yuwei Zeng, Jun Lv, Siyuan Luo, Qiancai Wang, Xinyuan Yu, Haonan Chen, Cewu Lu, and Lin Shao<br><br>IEEE/CVF International Conference on Computer Vision (ICCV) 2023 <br><br>
<a href="https://arxiv.org/pdf/2308.09987.pdf/">paper</a>
&nbsp;<a href="https://sites.google.com/view/clothesnet/">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/dexRepGrasp.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/2303.09806.pdf">DexRepNet: Learning Dexterous Robotic Grasping Network with
Geometric and Spatial Hand-Object Representations</a></b><br><br> Qingtao Liu*, Yu Cui*, Qi Ye, Zhengnan Sun, Haoming Li, Gaofeng Li, Lin Shao, Jiming Chen<br><br>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2023 <br><br>
<a href="https://arxiv.org/pdf/2303.09806.pdf">paper</a>
&nbsp;project
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/cloth_teaser.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/diffsimcloth/">DiffClothAI: Differentiable Cloth Simulation with Intersection-free
Frictional Contact and Differentiable Two-Way Coupling with Articulated Rigid Bodies</a></b><br><br> Xinyuan Yu*, Siheng Zhao*, Siyuan Luo, Gang Yang, and Lin Shao<br><br>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2023 <br><br>
<a href="https://linsats.github.io/">paper</a>
&nbsp;<a href="https://sites.google.com/view/diffsimcloth/">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./files/samrl.png" width="180" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/sam-rl">SAM-RL: Sensing-Aware Model-based Reinforcement Learning via Differentiable Physics-based Simulation and Rendering</a></b><br><br>Jun Lv, Yunhai Feng, Cheng Zhang, Shuang Zhao, Lin Shao* and Cewu Lu*<br><br>Proceedings of Robotics: Science and Systems (RSS) 2023

   <br><br> The International Journal of Robotics Research (IJRR)<br><br>
      <b style="color: red; ">Best System Paper Award finalist </b>  <br><br>
<a href="https://arxiv.org/abs/2210.15185">paper</a>
&nbsp;<a href="https://sites.google.com/view/sam-rl">project</a>
&nbsp;<a href="https://sites.google.com/view/sam-rl">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/sgci.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/egci">SAGCI-System: Towards Sample-Efficient, Generalizable, Compositional, and Incremental Robot Learning</a></b><br><br>Jun Lv*, Qiaojun Yu*, Lin Shao*, Wenhai Liu, Wenqiang Xu and Cewu Lu<br><br>IEEE International Conference on Robotics and Automation (ICRA) 2022 <br><br>
<a href="https://arxiv.org/abs/2111.14693">paper</a>
&nbsp;<a href="https://sites.google.com/view/egci">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=V3rcTVBktec">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/roboAssembly.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/roboticassembly">RoboAssembly:  Learning  Generalizable  Furniture  Assembly  Policy in  a  Novel  Multi-robot  Contact-rich  Simulation  Environment</a></b><br><br>Mingxin Yu*, Lin Shao*, Zhehuan Chen, Tianhao Wu, Qingnan Fan, Kaichun Mo, and Hao Dong<br><br>Arxiv Preprint, 2021 <br><br>
<a href="https://arxiv.org/abs/2112.10143">paper</a>
&nbsp;<a href="https://sites.google.com/view/roboticassembly">project</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/regrasp.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/regrasp">Learning to Regrasp by Learning to Place</a></b><br><br>Shuo Cheng, Kaichun Mo, Lin Shao<br><br>Conference on Robot Learning (CoRL) 2021<br><br>
<a href="https://arxiv.org/pdf/2109.08817.pdf">paper</a>
&nbsp;<a href="https://sites.google.com/view/regrasp">project</a>
&nbsp;<a href="https://github.com/touristCheng/Learning2Regrasp">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>



<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/DTAC.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/gracdrl">GRAC: Self-Guided and Self-Regularized Actor-Critic</a></b><br><br>Lin Shao, Yifan You, Mengyuan Yan, Shenli Yuan, Qingyun Sun, Jeannette Bohg<br><br>Conference on Robot Learning (CoRL) 2021<br><br>
<a href="https://arxiv.org/pdf/2009.08973.pdf">paper</a>
&nbsp;<a href="https://sites.google.com/view/gracdrl">project</a>
&nbsp;<a href="https://github.com/stanford-iprl-lab/GRAC">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/hangv2.png" width="170" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/hangingobject">OmniHang: Learning to Hang Arbitrary Objects Using Contact Point Correspondences and Neural Collision Estimation </a></b><br><br>Yifan You*, Lin Shao*, Toki Migimatsu, Jeannette Bohg<br><br>IEEE International Conference on Robotics and Automation (ICRA) 2021<br><br>
<a href="https://arxiv.org/abs/2103.14283">paper</a>
&nbsp;<a href="https://sites.google.com/view/hangingobject">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=yurtzk6Jy7o&ab_channel=LinShao">video</a> 
&nbsp;<a href="https://github.com/yifan-you-37/omnihang">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>




<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/partAssembly2.png" width="170" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/abs/2006.07793">	
Generative 3D Part Assembly via Dynamic Graph Learning</a></b><br><br>Jialei Huang*, Guanqi Zhan*, Qingnan Fan, Kaichun Mo, Lin Shao, Baoquan Chen, Leonidas J. Guibas, Hao Dong<br><br>NeurIPS 2020<br><br>
<a href="https://arxiv.org/abs/2006.07793">paper</a>
&nbsp;<a href="https://hyperplane-lab.github.io/Generative-3D-Part-Assembly/">project</a>
&nbsp;<a href="https://github.com/Championchess/Generative-3D-Part-Assembly">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/conceptlearning.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/concept2robot">Concept2Robot: Learning Manipulation Concepts from Instructions and Human Demonstrations</a></b><br><br>Lin Shao, Toki Migimatsu, Qiang Zhang,  Karen Yang, Jeannette Bohg<br><br>Proceedings of Robotics: Science and Systems (RSS) 2020<br><br>
    The International Journal of Robotics Research (IJRR)<br><br>
<a href="http://www.roboticsproceedings.org/rss16/p082.pdf">paper</a>
&nbsp;<a href="https://sites.google.com/view/concept2robot">project</a>
&nbsp;<a href="https://sites.google.com/view/concept2robot">video</a>
&nbsp;<a href="https://www.youtube.com/watch?v=flxrirLbxzg">presentation</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>


<!--Part Assembly-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/partAssembly.png" width="225" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/abs/2003.09754">Learning 3D Part Assembly from a Single Image</a></b><br><br>Yichen Li*, Kaichun Mo*, Lin Shao, Minhyuk Sung, Leonidas J. Guibas<br><br>ECCV 2020<br><br>
<a href="https://arxiv.org/abs/2003.09754">paper</a>
&nbsp;<a href="https://arxiv.org/abs/2003.09754">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=gtaBaEAs22s">presentation</a>
&nbsp;<a href="https://github.com/AntheaLi/3DPartAssembly">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!-- Roller Grasper-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/rollerGrasper.png" height="145" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/2004.08499.pdf">Design and Control of Roller Grasper V2 for In-Hand Manipulation</a></b><br><br>Shenli Yuan, Lin Shao, Connor L. Yako, Alex Gruebele, and J. Kenneth Salisbury<br><br>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2020<br><br>
<a href="https://arxiv.org/pdf/2004.08499.pdf">paper</a>
&nbsp;<a href="https://ccrma.stanford.edu/~shenliy/roller_grasper_v2.html">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=r_HaJfANyT8">video</a>
&nbsp;<a href="https://github.com/yuanshenli/RollerGrasperV2">code</a>
&nbsp;<a href="https://spectrum.ieee.org/automaton/robotics/robotics-hardware/we-can-do-better-than-humanlike-hands-for-robots">Featured in IEEE Spectrum</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!-- Scaffolding-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/scaffold.png" height="140" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/1911.00969.pdf">Learning to Scaffold the Development of Robotic Manipulation Skills</a></b><br><br>Lin Shao, Toki Migimatsu, Jeannette Bohg<br><br>IEEE International Conference on Robotics and Automation (ICRA) 2020<br><br>
<a href="https://arxiv.org/pdf/1911.00969.pdf">paper</a>
&nbsp;<a href="https://sites.google.com/view/scaffoldlearning">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=od3jBAJES4w">video</a>
&nbsp;<a href="https://github.com/stanford-iprl-lab/ScaffoldLearning">code</a>
&nbsp;<a href="https://www.youtube.com/watch?v=w-O6dxdVMpY&feature=youtu.be&ab_channel=LinShao">presentation</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!-- UniGrasp-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/metagrasp.png" height="140" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/1910.10900.pdf">UniGrasp: Learning a Unified Model to Grasp with Multifingered Robotic Hands</a></b><br><br>Lin Shao,  Fabio Ferreira*, Mikael Jorda*, Varun Nambiar*, Jianlan Luo, Eugen Solowjow, Juan Aparicio Ojea, Oussama Khatib, Jeannette Bohg<br><br>IEEE Robotics and Automation Letters with ICRA 2020 option<br><br>
<a href="https://arxiv.org/pdf/1910.10900.pdf">paper</a>
&nbsp;<a href="https://sites.google.com/view/unigrasp">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=UqVXL9QDnPU">video</a>
&nbsp;<a href="https://github.com/stanford-iprl-lab/UniGrasp">code</a>
&nbsp;<a href="https://www.youtube.com/watch?v=wv9h1ADJfDE&ab_channel=LinShao">presentation</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

</tbody>


<!-- Scene Flow -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/sceneflownet.png" height="140" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/1804.05195.pdf">Motion-based Object Segmentation 
based on Dense RGB-D Scene Flow</a></b><br><br>Lin Shao, Parth Shah<sup>*</sup>, Vikranth Dwaracherla<sup>*</sup>, Jeannette Bohg<br><br>IEEE Robotics and Automation Letters with IROS 2018 option<br><br>
<a href="https://arxiv.org/pdf/1804.05195.pdf">paper</a>
&nbsp;<a href="https://stanford-iprl-lab.github.io/sceneflownet/">project</a>
&nbsp;<a href="https://youtu.be/adAMsLraq9o">video</a>
&nbsp;<a href="https://stanford-iprl-lab.github.io/sceneflownet/">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

</tbody>

<!-- Size Transferring -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/attributeTransfer.png" height="140" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://ieeexplore.ieee.org/abstract/document/8374619/">Cross-modal Attribute Transfer for Rescaling 3D Models</a></b><br><br>Lin Shao, Angel X. Chang, Hao Su, Manolis Savva, Leonidas J. Guibas<br><br>3DV 2017<br><br>
<a href="https://ieeexplore.ieee.org/abstract/document/8374619/">paper</a>
&nbsp;<a href="https://linsats.github.io/attributeTransferring/">project</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

</tbody>
</table></ul></div>



<a name="teaching"></a>
<h3>Teaching Experiences</h3>
<div class="mainsection">
<ul>
<li>
Teaching Assistant. Spring 2020-21: <a href="http://cs231n.stanford.edu/">Convolutional Neural Networks for Visual Recognition (CS231N)</a>
</li>
</ul>
<ul>
<li>
Teaching Assistant. Winter 2017-18: <a href="https://cs.stanford.edu/groups/manips/teaching/cs223a/">Introduction to Robotics (CS223A)</a>
</li>
</ul>
<ul>
<li>
Teaching Assistant. Spring 2016-17: <a href="http://graphics.stanford.edu/courses/cs468-17-spring/">Machine Learning for 3D Data (CS468)</a>
</li>
</ul>
</div>


<h3>Professional Activities</h3>
<div class="mainsection">
<ul>
<li>
Conference Reviewer: 
<ul><li>IEEE International Conference on Robotics and Automation (ICRA), 2020/19/18</li></ul> 
<ul><li>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020/19/18</li></ul> 
<ul><li>Proceedings of Robotics: Science and Systems (RSS), 2019</li></ul>  
<ul><li>International Symposium on Robotics Research (ISRR), 2019</li></ul> 
<ul><li>Conference on Robot Learning (CoRL)</li></ul> 
<ul><li>International Conference on Machine Learning (ICML)</li></ul> 
<ul><li>International Conference on Learning Representations (ICLR)</li></ul> 
</li>
</ul>
<ul>
<li>
Journal Reviewer:
<ul><li>IEEE Robotics and Automation Letters (RA-L) 2020/19/18</li></ul>
<ul><li>IEEE Transactions on Robotics (T-RO)</li></ul>    
<ul><li>IEEE Transactions on Cognitive and Developmental Systems (TCDS)</li></ul>   
</li>
</ul>
</div>

</body><div></div></html>
