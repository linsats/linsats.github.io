<html><head><meta http-equiv="Content-Type" content="text/html; charset=GBK">
<title>Lin Shao</title>
<link rel="stylesheet" type="text/css" href="./files/main.css">
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CLMERFHZCQ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CLMERFHZCQ');
</script>
</head>


<body>

<table>
<tbody><tr>
<td><img src="./files/lins2.png" width="200"></td>
<td>
<div style="font-size:24; font-weight:bold">Lin Shao</div>
<div>
Assistant Professor at<br>
<a href="https://www.comp.nus.edu.sg/cs/"> Department of Computer Science</a>,<a href="https://www.comp.nus.edu.sg/"> School of Computing</a><br>
<a href="https://www.nus.edu.sg/">National University of Singapore</a><br>
</div>
<div>
<b>Email:</b> <tt>linshao at nus dot edu dot sg</tt><br>
<b>Office:</b>COM2-03-03<br>
</div>
<div>
<a href="https://linsats.github.io/#publications">[Publications]</a>&nbsp;
<a href="">[Experiences]</a>&nbsp;
<a href="https://linsats.github.io/#teaching">[Teaching]</a>&nbsp;
<a href="">[Misc]</a>&nbsp;
<br><br>
<a href="https://scholar.google.com/citations?user=UU76Pg4AAAAJ&hl=en">[Google Scholar]</a>
<a href="https://github.com/linsats">[GitHub]</a>
</div>
</td>
</tr>
</tbody></table>

<script type="text/javascript">
function hideshow(which){
if (!document.getElementById)
return
if (which.style.display=="block")
which.style.display="none"
else
which.style.display="block"
}
</script>


<h3>About</h3>
<div class="section">
<ul>
My research interests lie at the intersection of robotics and artificial intelligence. My long-term goal is to holistically build general-purpose robotic systems that intelligently perform a diverse range of tasks in a large variety of environments in the physical world. Specifically, my group is interested in developing algorithms and systems to provide robots with the abilities of perception and manipulation.<br><br>

I received my Ph.D. from Stanford University, advised by Professor <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a> and co-advised by Professor <a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a>. Prior to joining Stanford,  I was an undergraduate at <a href="https://www.nju.edu.cn/EN//">Nanjing University</a>.</ul>
</div>

<h4>PhD Dissertation Committee</h4>
<div class="subsubsection">
<ul>
<a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a> (Advisor)<br>
<a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a> (Co-advisor)<br>
<a href="https://khatib.stanford.edu/">Oussama Khatib</a>  <br>
<a href="https://sr.stanford.edu/?page_id=1217">J. Kenneth Salisbury</a>  <br>
<a href="http://bdml.stanford.edu/Profiles/MarkCutkosky">Mark Cutkosky</a> (Chair) <br>
<ul>
</div>


<h3>News</h3>
<div class="section">
 <li> <b style="color: red; ">NEW</b> [Aug, 2023]  <b style="color: red; ">Diff-LfD was accepted to CoRL 2023 as Oral!</b>
 <li> <b style="color: red; ">NEW</b> [June, 2023] ClothesNet was accepted to ICCV 2023!
 <li> <b style="color: red; ">NEW</b> [June, 2023]  <b style="color: red; ">SAM-RL wons Best System Paper Award finalist at RSS 2023!</b> 
 <li> <b style="color: red; ">NEW</b> [June, 2023] Two papers was accepted to IROS 2023!
<li> <b style="color: red; ">NEW</b> [April, 2023] Our SAM-RL paper was accepted to RSS 2023!
<li> <b style="color: green; background-color: #ffff42"></b> [Jan, 2022] Our Sagci-System paper was accepted to ICRA 2022!
<li> <b style="color: green; background-color: #ffff42"></b> [Dec, 2021] Our workshop Reinforcement Learning for Contact-Rich Manipulation was accepted to ICRA 2022!
</ul>
</div>


<h3>Joining My Group</h3>
<div class="section">
<ul>
<b style="color: green">I am looking for student researchers and postdocs to join my group at NUS. My group also has multiple positions for summer interns and visiting students. Please feel free to send me an email with your CV if you are interested in doing research with me. </b> <br>
</ul></div>    

<h3>Students & Interns</h3>
<ul><li>Gang Yang (Peking University)</li></ul>
<ul><li>Jinghan Ke (University of Science and Technology of China)</li></ul>
<ul><li>Haonan Chen (Nanjing University)</li></ul> 
<ul><li>Siyuan Luo (Xi'an Jiaotong University)</li></ul>
<ul><li>Weikun Peng (Beihang University)</li></ul>
<ul><li>Yuwei Zeng (Nanyang Technological University)</li></ul>
<ul><li>Qinsi Wang (University of Science and Technology of China)</li></ul>
<ul><li>Zhixuan Xu (Zhejiang University, Chu Kochen Honor College)</li></ul>
<ul><li>Zihao Xu (Tsinghua University)</li></ul>
<ul><li>Ce Hao (University of California, Berkeley)</li></ul>
<ul><li>Haoyu Zhou (Beihang University)</li></ul>
<ul><li>Tianyi Chen (Shanghai Jiao Tong University)</li></ul>
<ul><li>Jichen Sun (Shanghai Jiao Tong University, IEEE Honor Class)</li></ul>
<ul><li>Chenrui Tie (Peking University)</li></ul>
<ul><li>Siyuan Zhang (Tsinghua University)</li></ul>
<ul><li>Chen Yu (Tsinghua University)</li></ul>
<ul><li>Jingyuan Cong (Zhejiang University, Chu Kochen Honor College)</li></ul>
<ul><li>Debang Wang (National University of Singapore)</li></ul>
<ul><li>Jiajun Fan (Tsinghua University)</li></ul>
<ul><li>Chongkai Gao (Tsinghua University)</li></ul>

<h3>Alumni</h3>
<ul><li>Mingxin Yu (Peking University), Next Ph.D. student at MIT</li></ul> 
<ul><li>Yifan You (UCLA), Next Ph.D. student at Berkeley</ul> 
<ul><li>Shuo Cheng (UCSD), Next Ph.D. student at Georgia Tech</li></ul>
<ul><li>Ruiqi Zhang (Tongji University), Next Ph.D. student at Berkeley</li></ul>

<a name="publications"></a>
<h3>Recent Publications</h3>
<div class="mainsection">
<ul>
<table width="100%">





<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/adamas.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/diffsim/">Adamas: A Differentiable Physics Engine for Articulated
Rigid Bodies with Intersection-Free Frictional Contact</a></b><br><br> Gang Yang, Siyuan Luo, Lin Shao<br><br>
<a href="https://linsats.github.io/">paper</a>
&nbsp;<a href="https://sites.google.com/view/diffsim/">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/difflfd.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/diff-lfd/">Diff-LfD: Contact-aware Model-based Learning from Visual
Demonstration for Robotic Manipulation via Differentiable Physics-based
Simulation and Rendering</a></b><br><br> Xinghao Zhu, Jinghan Ke, Zhixuan Xu, Zhixin Sun, Bizhe Bai, Jun Lv, Qingtao Liu, Yuwei Zeng, Qi Ye, Cewu Lu, Masayoshi Tomizuka, Lin Shao<br><br>Conference on Robot Learning (CoRL) 2023  <b style="color: red; ">(Oral)<br><br>
<a href="https://linsats.github.io/">paper</a>
&nbsp;<a href="https://sites.google.com/view/diff-lfd">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/clothesnet.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/clothesnet/">ClothesNet: An Information-Rich 3D Garment Model Repository with Simulated Clothes Environment</a></b><br><br> Bingyang Zhou, Haoyu Zhou, Tianhai Liang, Qiaojun Yu, Siheng Zhao, Yuwei Zeng, Jun Lv, Siyuan Luo, Qiancai Wang, Xinyuan Yu, Haonan Chen, Cewu Lu, and Lin Shao<br><br>IEEE/CVF International Conference on Computer Vision (ICCV) 2023 <br><br>
<a href="https://arxiv.org/pdf/2308.09987.pdf/">paper</a>
&nbsp;<a href="https://sites.google.com/view/clothesnet/">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/dexRepGrasp.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/2303.09806.pdf">DexRepNet: Learning Dexterous Robotic Grasping Network with
Geometric and Spatial Hand-Object Representations</a></b><br><br> Qingtao Liu*, Yu Cui*, Qi Ye, Zhengnan Sun, Haoming Li, Gaofeng Li, Lin Shao, Jiming Chen<br><br>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2023 <br><br>
<a href="https://arxiv.org/pdf/2303.09806.pdf">paper</a>
&nbsp;project
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/cloth_teaser.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/diffsimcloth/">DiffClothAI: Differentiable Cloth Simulation with Intersection-free
Frictional Contact and Differentiable Two-Way Coupling with Articulated Rigid Bodies</a></b><br><br> Xinyuan Yu*, Siheng Zhao*, Siyuan Luo, Gang Yang, and Lin Shao<br><br>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2023 <br><br>
<a href="https://linsats.github.io/">paper</a>
&nbsp;<a href="https://sites.google.com/view/diffsimcloth/">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./files/samrl.png" width="180" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/sam-rl">SAM-RL: Sensing-Aware Model-based Reinforcement Learning via Differentiable Physics-based Simulation and Rendering</a></b><br><br>Jun Lv, Yunhai Feng, Cheng Zhang, Shuang Zhao, Lin Shao* and Cewu Lu*<br><br>Proceedings of Robotics: Science and Systems (RSS) 2023<br><br>  <b style="color: red; ">Best System Paper Award finalist </b>  <br><br>
<a href="https://arxiv.org/abs/2210.15185">paper</a>
&nbsp;<a href="https://sites.google.com/view/sam-rl">project</a>
&nbsp;<a href="https://sites.google.com/view/sam-rl">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/sgci.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/egci">SAGCI-System: Towards Sample-Efficient, Generalizable, Compositional, and Incremental Robot Learning</a></b><br><br>Jun Lv*, Qiaojun Yu*, Lin Shao*, Wenhai Liu, Wenqiang Xu and Cewu Lu<br><br>IEEE International Conference on Robotics and Automation (ICRA) 2022 <br><br>
<a href="https://arxiv.org/abs/2111.14693">paper</a>
&nbsp;<a href="https://sites.google.com/view/egci">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=V3rcTVBktec">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/roboAssembly.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/roboticassembly">RoboAssembly:  Learning  Generalizable  Furniture  Assembly  Policy in  a  Novel  Multi-robot  Contact-rich  Simulation  Environment</a></b><br><br>Mingxin Yu*, Lin Shao*, Zhehuan Chen, Tianhao Wu, Qingnan Fan, Kaichun Mo, and Hao Dong<br><br>Arxiv Preprint, 2021 <br><br>
<a href="https://arxiv.org/abs/2112.10143">paper</a>
&nbsp;<a href="https://sites.google.com/view/roboticassembly">project</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/regrasp.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/regrasp">Learning to Regrasp by Learning to Place</a></b><br><br>Shuo Cheng, Kaichun Mo, Lin Shao<br><br>Conference on Robot Learning (CoRL) 2021<br><br>
<a href="https://arxiv.org/pdf/2109.08817.pdf">paper</a>
&nbsp;<a href="https://sites.google.com/view/regrasp">project</a>
&nbsp;<a href="https://github.com/touristCheng/Learning2Regrasp">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>



<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/DTAC.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/gracdrl">GRAC: Self-Guided and Self-Regularized Actor-Critic</a></b><br><br>Lin Shao, Yifan You, Mengyuan Yan, Shenli Yuan, Qingyun Sun, Jeannette Bohg<br><br>Conference on Robot Learning (CoRL) 2021<br><br>
<a href="https://arxiv.org/pdf/2009.08973.pdf">paper</a>
&nbsp;<a href="https://sites.google.com/view/gracdrl">project</a>
&nbsp;<a href="https://github.com/stanford-iprl-lab/GRAC">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/hangv2.png" width="170" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/hangingobject">OmniHang: Learning to Hang Arbitrary Objects Using Contact Point Correspondences and Neural Collision Estimation </a></b><br><br>Yifan You*, Lin Shao*, Toki Migimatsu, Jeannette Bohg<br><br>IEEE International Conference on Robotics and Automation (ICRA) 2021<br><br>
<a href="https://arxiv.org/abs/2103.14283">paper</a>
&nbsp;<a href="https://sites.google.com/view/hangingobject">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=yurtzk6Jy7o&ab_channel=LinShao">video</a> 
&nbsp;<a href="https://github.com/yifan-you-37/omnihang">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>




<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/partAssembly2.png" width="170" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/abs/2006.07793">	
Generative 3D Part Assembly via Dynamic Graph Learning</a></b><br><br>Jialei Huang*, Guanqi Zhan*, Qingnan Fan, Kaichun Mo, Lin Shao, Baoquan Chen, Leonidas J. Guibas, Hao Dong<br><br>NeurIPS 2020<br><br>
<a href="https://arxiv.org/abs/2006.07793">paper</a>
&nbsp;<a href="https://hyperplane-lab.github.io/Generative-3D-Part-Assembly/">project</a>
&nbsp;<a href="https://github.com/Championchess/Generative-3D-Part-Assembly">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/conceptlearning.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/concept2robot">Concept2Robot: Learning Manipulation Concepts from Instructions and Human Demonstrations</a></b><br><br>Lin Shao, Toki Migimatsu, Qiang Zhang,  Karen Yang, Jeannette Bohg<br><br>Proceedings of Robotics: Science and Systems (RSS) 2020<br><br>
    The International Journal of Robotics Research (IJRR)<br><br>
<a href="http://www.roboticsproceedings.org/rss16/p082.pdf">paper</a>
&nbsp;<a href="https://sites.google.com/view/concept2robot">project</a>
&nbsp;<a href="https://sites.google.com/view/concept2robot">video</a>
&nbsp;<a href="https://www.youtube.com/watch?v=flxrirLbxzg">presentation</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>


<!--Part Assembly-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/partAssembly.png" width="225" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/abs/2003.09754">Learning 3D Part Assembly from a Single Image</a></b><br><br>Yichen Li*, Kaichun Mo*, Lin Shao, Minhyuk Sung, Leonidas J. Guibas<br><br>ECCV 2020<br><br>
<a href="https://arxiv.org/abs/2003.09754">paper</a>
&nbsp;<a href="https://arxiv.org/abs/2003.09754">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=gtaBaEAs22s">presentation</a>
&nbsp;<a href="https://github.com/AntheaLi/3DPartAssembly">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!-- Roller Grasper-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/rollerGrasper.png" height="145" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/2004.08499.pdf">Design and Control of Roller Grasper V2 for In-Hand Manipulation</a></b><br><br>Shenli Yuan, Lin Shao, Connor L. Yako, Alex Gruebele, and J. Kenneth Salisbury<br><br>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2020<br><br>
<a href="https://arxiv.org/pdf/2004.08499.pdf">paper</a>
&nbsp;<a href="https://ccrma.stanford.edu/~shenliy/roller_grasper_v2.html">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=r_HaJfANyT8">video</a>
&nbsp;<a href="https://github.com/yuanshenli/RollerGrasperV2">code</a>
&nbsp;<a href="https://spectrum.ieee.org/automaton/robotics/robotics-hardware/we-can-do-better-than-humanlike-hands-for-robots">Featured in IEEE Spectrum</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!-- Scaffolding-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/scaffold.png" height="140" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/1911.00969.pdf">Learning to Scaffold the Development of Robotic Manipulation Skills</a></b><br><br>Lin Shao, Toki Migimatsu, Jeannette Bohg<br><br>IEEE International Conference on Robotics and Automation (ICRA) 2020<br><br>
<a href="https://arxiv.org/pdf/1911.00969.pdf">paper</a>
&nbsp;<a href="https://sites.google.com/view/scaffoldlearning">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=od3jBAJES4w">video</a>
&nbsp;<a href="https://github.com/stanford-iprl-lab/ScaffoldLearning">code</a>
&nbsp;<a href="https://www.youtube.com/watch?v=w-O6dxdVMpY&feature=youtu.be&ab_channel=LinShao">presentation</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!-- UniGrasp-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/metagrasp.png" height="140" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/1910.10900.pdf">UniGrasp: Learning a Unified Model to Grasp with Multifingered Robotic Hands</a></b><br><br>Lin Shao,  Fabio Ferreira*, Mikael Jorda*, Varun Nambiar*, Jianlan Luo, Eugen Solowjow, Juan Aparicio Ojea, Oussama Khatib, Jeannette Bohg<br><br>IEEE Robotics and Automation Letters with ICRA 2020 option<br><br>
<a href="https://arxiv.org/pdf/1910.10900.pdf">paper</a>
&nbsp;<a href="https://sites.google.com/view/unigrasp">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=UqVXL9QDnPU">video</a>
&nbsp;<a href="https://github.com/stanford-iprl-lab/UniGrasp">code</a>
&nbsp;<a href="https://www.youtube.com/watch?v=wv9h1ADJfDE&ab_channel=LinShao">presentation</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

</tbody>


<!-- Scene Flow -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/sceneflownet.png" height="140" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/1804.05195.pdf">Motion-based Object Segmentation 
based on Dense RGB-D Scene Flow</a></b><br><br>Lin Shao, Parth Shah<sup>*</sup>, Vikranth Dwaracherla<sup>*</sup>, Jeannette Bohg<br><br>IEEE Robotics and Automation Letters with IROS 2018 option<br><br>
<a href="https://arxiv.org/pdf/1804.05195.pdf">paper</a>
&nbsp;<a href="https://stanford-iprl-lab.github.io/sceneflownet/">project</a>
&nbsp;<a href="https://youtu.be/adAMsLraq9o">video</a>
&nbsp;<a href="https://stanford-iprl-lab.github.io/sceneflownet/">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

</tbody>

<!-- Size Transferring -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/attributeTransfer.png" height="140" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://ieeexplore.ieee.org/abstract/document/8374619/">Cross-modal Attribute Transfer for Rescaling 3D Models</a></b><br><br>Lin Shao, Angel X. Chang, Hao Su, Manolis Savva, Leonidas J. Guibas<br><br>3DV 2017<br><br>
<a href="https://ieeexplore.ieee.org/abstract/document/8374619/">paper</a>
&nbsp;<a href="https://linsats.github.io/attributeTransferring/">project</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

</tbody>
</table></ul></div>



<a name="teaching"></a>
<h3>Teaching Experiences</h3>
<div class="mainsection">
<ul>
<li>
Teaching Assistant. Spring 2020-21: <a href="http://cs231n.stanford.edu/">Convolutional Neural Networks for Visual Recognition (CS231N)</a>
</li>
</ul>
<ul>
<li>
Teaching Assistant. Winter 2017-18: <a href="https://cs.stanford.edu/groups/manips/teaching/cs223a/">Introduction to Robotics (CS223A)</a>
</li>
</ul>
<ul>
<li>
Teaching Assistant. Spring 2016-17: <a href="http://graphics.stanford.edu/courses/cs468-17-spring/">Machine Learning for 3D Data (CS468)</a>
</li>
</ul>
</div>


<h3>Professional Activities</h3>
<div class="mainsection">
<ul>
<li>
Conference Reviewer: 
<ul><li>IEEE International Conference on Robotics and Automation (ICRA), 2020/19/18</li></ul> 
<ul><li>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020/19/18</li></ul> 
<ul><li>Proceedings of Robotics: Science and Systems (RSS), 2019</li></ul>  
<ul><li>International Symposium on Robotics Research (ISRR), 2019</li></ul> 
<ul><li>Conference on Robot Learning (CoRL)</li></ul> 
<ul><li>International Conference on Machine Learning (ICML)</li></ul> 
<ul><li>International Conference on Learning Representations (ICLR)</li></ul> 
</li>
</ul>
<ul>
<li>
Journal Reviewer:
<ul><li>IEEE Robotics and Automation Letters (RA-L) 2020/19/18</li></ul>
<ul><li>IEEE Transactions on Robotics (T-RO)</li></ul>    
<ul><li>IEEE Transactions on Cognitive and Developmental Systems (TCDS)</li></ul>   
</li>
</ul>
</div>

</body><div></div></html>
