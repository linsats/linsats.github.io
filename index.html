<html><head><meta http-equiv="Content-Type" content="text/html; charset=GBK">
<title>Lin Shao</title>
<link rel="stylesheet" type="text/css" href="./files/main.css">
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CLMERFHZCQ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CLMERFHZCQ');
</script>
</head>


<body>

<table>
<tbody><tr>
<td><img src="./files/lins2.png" width="200"></td>
<td>
<div style="font-size:24; font-weight:bold">Lin Shao</div>
<div>
Assistant Professor at<br>
<a href="https://www.comp.nus.edu.sg/cs/"> Department of Computer Science</a>,<a href="https://www.comp.nus.edu.sg/"> School of Computing</a><br>
<a href="https://www.nus.edu.sg/">National University of Singapore</a><br>
</div>
<div>
<b>Email:</b> <tt>linshao at nus dot edu dot sg</tt><br>
<b>Office:</b>COM2-03-03<br>
</div>
<div>
<a href="https://linsats.github.io/#publications">[Publications]</a>&nbsp;
<a href="">[Experiences]</a>&nbsp;
<a href="https://linsats.github.io/#teaching">[Teaching]</a>&nbsp;
<a href="">[Misc]</a>&nbsp;
<br><br>
<a href="https://scholar.google.com/citations?user=UU76Pg4AAAAJ&hl=en">[Google Scholar]</a>
<a href="https://github.com/linsats">[GitHub]</a>
</div>
</td>
</tr>
</tbody></table>

<script type="text/javascript">
function hideshow(which){
if (!document.getElementById)
return
if (which.style.display=="block")
which.style.display="none"
else
which.style.display="block"
}
</script>


<h3>About</h3>
<div class="section">
<ul>
My research interests lie at the intersection of robotics and artificial intelligence. My long-term goal is to holistically build general-purpose robotic systems that intelligently perform a diverse range of tasks in a large variety of environments in the physical world. Specifically, my group is interested in developing algorithms and systems to provide robots with the abilities of perception and manipulation. I serve as a co-chair of Technical Committee on Robot Learning in the IEEE Robotics and Automation Society, and as an Associate Editor for IEEE Robotics and Automation Letters.<br><br>

I received my Ph.D. from Stanford University, advised by Professor <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a> and co-advised by Professor <a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a>. Prior to joining Stanford,  I was an undergraduate at <a href="https://www.nju.edu.cn/EN//">Nanjing University</a>.</ul>
</div>

<h4>PhD Dissertation Committee</h4>
<div class="subsubsection">
<ul>
<a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a> (Advisor)<br>
<a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a> (Co-advisor)<br>
<a href="https://khatib.stanford.edu/">Oussama Khatib</a>  <br>
<a href="https://sr.stanford.edu/?page_id=1217">J. Kenneth Salisbury</a>  <br>
<a href="http://bdml.stanford.edu/Profiles/MarkCutkosky">Mark Cutkosky</a> (Chair) <br>
<ul>
</div>

<h3>News</h3>

<div class="section"> 
<li> <b style="color: red; ">NEW</b> [May, 2025] I will serve as an Associate Editor of IEEE Transactions on Robotics (TR-O)!
<li> <b style="color: red; ">NEW</b> [May, 2025] Best Paper Award @ ICRA 2025 Workshop on Human-Centric Teleoperation!
<li> <b style="color: red; ">NEW</b> [May, 2025] D(R,O) Grasp won Best Paper Award on Robot Manipulation and Locomotion at ICRA 2025!   
<li> <b style="color: red; ">NEW</b> [April, 2025] D(R,O) Grasp is selected as a finalist for Best Paper Award and Best Paper Award on Robot Manipulation and Locomotion at ICRA 2025!
<li> <b style="color: red; ">NEW</b> [April, 2025] Manual2Skill is accepted to RSS 2025!
<li> <b style="color: red; ">NEW</b> [Nov, 2024] D(R,O) Grasp won Best Robotics Paper Award at  <a href="https://sites.google.com/view/corl-mapodel-workshop/schedule">CoRL workshop</a></b>  
<li> <b style="color: red; ">NEW</b> [Nov, 2024] Invited Talk at <a href="https://svl.stanford.edu/">Stanford Vision and Learning Lab</a> </b>  
<li> <b style="color: red; ">NEW</b> [Sept, 2024] Invited Talk at CoRL 2024 Workshop on Differentiable Optimization Everywhere: Simulation, Estimation, Learning, and Control </b>  
<li> <b style="color: red; ">NEW</b> [Sept, 2024] One paper was accepted to NeurIPS 2024</b>     
<li> <b style="color: red; ">NEW</b> [Sept, 2024] TieBot was accepted to CoRL 2024 as Oral</b>    
<li> [Sept, 2024] Two papers are accepted to CoRL 2024</b>    
<li> [Aug, 2024] SAM-RL is accepted to IJRR</b>     
<li> [Aug, 2024] I serve as an Associate Editor at ICRA 2025</b>     
<li> [July, 2024] Roller Grasper V3 was accepted to TR-O </b>     
<li> [July, 2024] Two papers were accepted to IROS 2024  </b>        
</ul>
</div>


<h3>Joining My Group</h3>
<div class="section">
<ul>
<b style="color: green">I am looking for student researchers and postdocs to join my group at NUS. My group also has multiple positions for summer interns and visiting students. Please feel free to send me an email with your CV if you are interested in doing research with me. </b> <br>


</ul></div>    

<h3>People</h3>

<h4>Postdoc</h4>
<ul><li><a href="https://djiajunustc.github.io//">Jiajun Deng</a> (University of Science and Technology of China)</li></ul>

<h4>PhD Students</h4>
<ul><li><a href="https://chongkaigao.com/">Chongkai Gao</a> (Tsinghua University)</li></ul>
<ul><li><a href="https://ariszxxu.github.io/">Zhixuan Xu</a> (Zhejiang University)</li></ul>
<ul><li><a href="https://crtie.github.io/">Chenrui Tie</a> (Peking University)</li></ul>
<ul><li><a href="https://panda-shawn.github.io/">Zixuan Liu</a> (Tsinghua University)</li></ul>
<ul><li><a href="https://sgtvincent.github.io">Junting Chen</a> (ETH Zurich)</li></ul>
<ul><li><a href="https://chenhn02.github.io/"> Haonan Chen</a> (Nanjing University)</li></ul>
<ul><li><a href="https://houyiwen.github.io/">Yiwen Hou</a> (University of Science and Technology of China)</li></ul>
<ul><li><a href="https://ztr583.github.io/">Tianrui Zhang</a> (Tsinghua University)</li></ul>
<ul><li><a href="https://xinfei21.github.io/web/">Fei Xin</a> (Tsinghua University)</li></ul>
<ul><li><a href="https://mrsecant.github.io/">Yuhang Zheng</a> (Beihang University)</li></ul>

<h3>Master's Students</h3>
<ul><li>Debang Wang (National University of Singapore)</li></ul>
<ul><li>Jiayu Luo (Beijing Institute of Technology)</li></ul>
<ul><li>Zhehao Cai (Nanjing University of Aeronautics and Astronautics)</li></ul>
<ul><li>Xiao Liu (Beihang University)</li></ul>
<ul><li>Zeyu Liu (Tongji University)</li></ul>
<ul><li>Jiayi Li (University of Nottingham)</li></ul>
<ul><li>Yunfan Lou (Shandong University)</li></ul>
<ul><li>Tongzhou Zhang (Tongji University)</li></ul>
<ul><li>Zhewei Gui (Tongji University)</li></ul> 
<ul><li>Haotian Shi (Beijing University of Posts and Telecommunications)</li></ul> 
<ul><li>Li Zhongrui (South China University of Technology)</li></ul>
<ul><li>Jinheng Li (East China Normal University)</li></ul>

<h3>Visiting Scholar</h3>
<ul><li>Zixuan Chen (Nanjing University, CSC funded)</li></ul>

<h3>Research Assistants</h3>
<ul><li>Xinyu Chen (Peking University)</li></ul>
<ul><li>Jinxuan Zhu (Harbin Institute of Technology, Shenzhen)</li></ul>
<ul><li>Jingxiang Guo (Harbin Institute of Technology, Shenzhen)</li></ul>
<ul><li>Junxiao Li (Nanjing University)</li></ul>
<ul><li>Xuanye Wu (Nanyang Technology University)</li></ul>
<ul><li>Zhaoyu Mu (National University of Singapore)</li></ul>
<ul><li>Xuchuan Huang (Peking University)</li></ul> 
<ul><li>Shengjia Zhu (Shanghai Jiaotong University)</li></ul>
<ul><li>Junbo Wang (Shanghai Jiaotong University)</li></ul>
<ul><li>Mohan Wang (Tongji University)</li></ul>
<ul><li>Shuo Zhang (Shanghai Jiaotong University)</li></ul>
<ul><li>Boren Zheng (Tsinghua University)</li></ul>
<ul><li>Yixin Cao (East China Normal University)</li></ul>
<ul><li>Yudi Lin (University of Southern California)</li></ul>
<ul><li>Shengsong Huang (Georgia Institute of Technology)</li></ul> 
<ul><li>Zhenghao Chi (University of Chicago)</li></ul>  
<ul><li>Yiman Pang (Jilin University)</li></ul>  
<ul><li>Jinfeng Wang (Jilin University)</li></ul>  
<ul><li>Zheng Boren (Tsinghua University)</li></ul> 
<ul><li>Yiwei Liu (Sichuan University)</li></ul> 
<ul><li>Shengxiang Sun (University of Toronto)</li></ul> 
<ul><li>Wanqi Zhong (Harbin Institute of Technology)</li></ul>
<ul><li>Huaicong Fang(Zhejiang University)</li></ul>

<h3>Alumni</h3>
<ul><li>Mingxin Yu (Peking University), Next Ph.D. student at MIT</li></ul> 
<ul><li>Yifan You (UCLA), Next Ph.D. student at Berkeley</ul> 
<ul><li>Shuo Cheng (UCSD), Next Ph.D. student at Georgia Tech</li></ul>
<ul><li>Ruiqi Zhang (Tongji University), Next Ph.D. student at Berkeley</li></ul>
<ul><li>Weikun Peng (Beihang University), Next Ph.D. student at Simon Fraser University</li></ul>
<ul><li>Zhenyu Wei (Shanghai Jiaotong University), Next Ph.D. student at University of North Carolina at Chapel Hill</li></ul>
<ul><li>Haoyu Zhou (Beihang University), Next Ph.D. student at Oxford</li></ul>
<ul><li>Xunzhe Zhou (Fudan University), Next Ph.D. student at The University of Hong Kong</li></ul>

<a name="publications"></a>
<h3>Recent Publications</h3>
<div class="mainsection">
<ul>
<table width="100%">



<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/dexsingrasp.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://nus-lins-lab.github.io/dexsingweb/">DexSinGrasp: Learning a Unified Policy for Dexterous Object Singulation and Grasping in Cluttered Environments</a></b><br><br>Lixin Xu*, Zixuan Liu*, Zhewei Gui, Jingxiang Guo, Zeyu Jiang, Zhixuan Xu, Chongkai Gao, Lin Shao<br><br>Arxiv<br><br>
<a href="https://arxiv.org/pdf/2504.04516">paper</a>
&nbsp;<a href="https://nus-lins-lab.github.io/dexsingweb/">project</a>
&nbsp;<a href="https://nus-lins-lab.github.io/dexsingweb/">video</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/MetaFold.png" width="170" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://meta-fold.github.io//">
MetaFold: Language-Guided Multi-Category Garment Folding Framework via Trajectory Generation and Foundation Model</a></b><br><br>Haonan Chen, Junxiao Li, Ruihai Wu, Yiwei Liu, Yiwen Hou, Zhixuan Xu, Jingxiang Guo, Chongkai Gao, Zhenyu Wei, Shensi Xu, Jiaqi Huang, Lin Shao<br><br>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2025<br><br>
<a href="https://arxiv.org/abs/2503.08372">paper</a>
&nbsp;<a href="https://meta-fold.github.io/">project</a>
&nbsp;<a href="https://meta-fold.github.io/">video</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>



<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/manual2skill.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://owensun2004.github.io/Furniture-Assembly-Web/">Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models</a></b><br><br> Chenrui Tie, Shengxiang Sun, Jinxuan Zhu, Yiwei Liu, Jingxiang Guo, Yue Hu, Haonan Chen, Junting Chen, Ruihai Wu, Lin Shao
<br><br>Proceedings of Robotics: Science and Systems (RSS) 2025<br><br>
<a href="https://arxiv.org/pdf/2502.10090">paper</a>
&nbsp;<a href="https://owensun2004.github.io/Furniture-Assembly-Web">project</a>
&nbsp;<a href="https://owensun2004.github.io/Furniture-Assembly-Web">video</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/TelePreview.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://nus-lins-lab.github.io/telepreview/">TelePreview: A User-Friendly Teleoperation System with
Virtual Arm Assistance for Enhanced Effectiveness</a></b><br><br> Jingxiang Guo, Jiayu Luo, Zhenyu Wei, Yiwen Hou, Zhixuan Xu, Xiaoyi Lin, Chongkai Gao, Lin Shao<br>
<br>Arxiv<br>
<br><b style="color: red; ">Best Paper Award @ ICRA 2025 Workshop on Human-Centric Teleoperation </b><br><br>
<a href="https://arxiv.org/abs/2412.13548">paper</a>
&nbsp;<a href="https://nus-lins-lab.github.io/telepreview-web/">project</a>
&nbsp;<a href="https://nus-lins-lab.github.io/telepreview-web/">video</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>



<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/emos.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://github.com/SgtVincent/EMOS">EMOS: Embodiment-aware Heterogeneous Multi-robot Operating System with LLM Agents</a></b><br><br> Junting Chen, Checheng Yu, Xunzhe Zhou, Tianqi Xu, Yao Mu, Mengkang Hu, Wenqi Shao, Yikai Wang, Guohao Li, Lin Shao
<br><br>International Conference on Learning Representations (ICLR) 2025<br><br>
<a href="https://arxiv.org/abs/2410.22662">paper</a>
&nbsp;<a href="https://github.com/SgtVincent/EMOS/">project</a>
&nbsp;video
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>



<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/flip.png" width="180" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://nus-lins-lab.github.io/flipweb/">FLIP: Flow-Centric Generative Planning as General-Purpose Manipulation World Model</a></b><br><br> Chongkai Gao, Haozhuo Zhang, Zhixuan Xu, Zhehao Cai, Lin Shao
<br><br> International Conference on Learning Representations (ICLR) 2025<br><br>
<a href="https://arxiv.org/abs/2412.08261">paper</a>
&nbsp;<a href="https://nus-lins-lab.github.io/flipweb/">project</a>
&nbsp;<a href="https://nus-lins-lab.github.io/flipweb//">video</a> 
&nbsp;<a href="https://github.com/HeegerGao/FLIP">code</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/drograsp.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://nus-lins-lab.github.io/drograspweb/">D(R,O) Grasp: A Unified Representation of Robot and Object Interaction for Cross-Embodiment Dexterous Grasping</a></b><br><br> Zhenyu Wei*, Zhixuan Xu*, Jingxiang Guo, Yiwen Hou, Chongkai Gao, Zhehao Cai, Jiayu Luo, Lin Shao
<br><br>IEEE International Conference on Robotics and Automation (ICRA) 2025 
<br><br>      <b style="color: red; ">Best Robotics Paper Award at CoRL MAPoDeL Workshop </b>
<br><br>      <b style="color: red; ">Best Paper Award finalist at ICRA 2025 </b> 
<br><br>      <b style="color: red; ">Best Paper Award on Robot Manipulation and Locomotion at ICRA 2025 </b>
 <br><br>
<a href="https://arxiv.org/abs/2410.01702">paper</a>
&nbsp;<a href="https://nus-lins-lab.github.io/drograspweb/">project</a>
&nbsp;<a href="https://nus-lins-lab.github.io/drograspweb/">video</a> 
&nbsp;<a href="https://github.com/zhenyuwei2003/DRO-Grasp">code</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/tiebot.png" width="140" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/tiebot">TieBot: Model-based Learning to Knot a Tie from Visual
Demonstration via Differentiable Physics-based Simulation</a></b><br><br> Weikun Peng, Jun Lv, Yuwei Zeng, Haonan Chen, Siheng Zhao, Jichen Sun, Cewu Lu, and Lin Shao
<br><br>Conference on Robot Learning (CoRL) 2024 <b style="color: red; ">(Oral)<br><br>
<a href="https://arxiv.org/pdf/2407.03245">paper</a>
&nbsp;<a href="https://tiebots.github.io/">project</a>
&nbsp;<a href="https://tiebots.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/SE3.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://riemann-web.github.io/">RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation</a></b><br><br> Chongkai Gao, Zhengrong Xue, Shuying Deng, Tianhai Liang, Siqi Yang, Lin Shao, Huazhe Xu
<br><br>Conference on Robot Learning (CoRL) 2024 <br><br>
<a href="https://arxiv.org/abs/2403.19460">paper</a>
&nbsp;<a href="https://riemann-web.github.io/">project</a>
&nbsp;<a href="https://riemann-web.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>
    

<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/rollerv3.png" height="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://yuanshenli.com/roller_grasper_v3.html">Design and Control of Roller Grasper V3 for
In-Hand Manipulation</a></b><br><br> Shenli Yuan, Lin Shao, Yunhai Feng, Jiatong Sun, Teng Xue, Connor
Yako, Jeannette Bohg, Kenneth Salisbury<br><br>
IEEE Transactions on Robotics<br><br>
<a href="https://ieeexplore.ieee.org/document/10666738">paper</a>
&nbsp;<a href="https://yuanshenli.com/roller_grasper_v3.html">project</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody> 



<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/manifm.png" height="140" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://manifoundationmodel.github.io/">ManiFoundation Model for General-Purpose Robotic Manipulation of Contact Synthesis with Arbitrary Objects and Robots</a></b><br><br> Zhixuan Xu*, Chongkai Gao*, Zixuan Liu*, Gang Yang*, Chenrui Tie, Haozhuo Zheng, Haoyu Zhou, Weikun Peng, Debang Wang, Tianyi Chen, Zhouliang Yu, Lin Shao<br><br>
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2024 <b style="color: red; ">(Oral)<br><br>
<a href="https://arxiv.org/pdf/2405.06964">paper</a>
&nbsp;<a href="https://manifoundationmodel.github.io/">project</a>
&nbsp;<a href="https://manifoundationmodel.github.io/">video</a> 
&nbsp;<a href="https://github.com/NUS-LinS-Lab/ManiFM">code</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/mpm.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://linsats.github.io">SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact
Model and Two-way Coupling with Articulated Rigid Bodies and Clothes</a></b><br><br> Min Liu, Gang Yang, Siyuan Luo, Chen Yu, and Lin Shao<br><br>
  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2024 <br><br>
<a href="https://linsats.github.io">paper</a>
&nbsp;<a href="https://sites.google.com/view/softmac">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/learning2rank.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/rewardselfalign">Learning Reward for Robot Skills Using Large Language Models via
Self-Alignment</a></b><br><br>Yuwei Zeng, Yao Mu, Lin Shao <br><br>
 International Conference on Machine Learning (ICML) 2024<br><br>
<a href="https://arxiv.org/pdf/2405.07162">paper</a>
&nbsp;<a href="https://sites.google.com/view/rewardselfalign/">project</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/llm.png" width="145" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="ttps://object814.github.io/Task-Condition-With-LLM/">Generalizable Long-Horizon Manipulations
with Large Language Models</a></b><br><br> Haoyu Zhou, Mingyu Ding, Weikun Peng, Masayoshi Tomizuka, Lin Shao and Chuang Gan<br><br>
Arxiv <br><br>
<a href="https://arxiv.org/pdf/2310.02264v1.pdf">paper</a>
&nbsp;<a href="https://object814.github.io/Task-Condition-With-LLM/">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>








<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/m3dpart.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/cloth-grasping">Category-Level Multi-Part Multi-Joint 3D Shape Assembly</a></b><br><br>Yichen Li, Kaichun Mo, Yueqi Duan, He Wang, Jiequan Zhang, Lin Shao, Wojciech Matusik and Leonidas J. Guibas<br><br>
<a href="https://linsats.github.io">IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024 (CVPR)<br><br>
&nbsp;<a href="https://sites.google.com/view/cloth-grasping">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/adamas.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/diffsim/">Jade: A Differentiable Physics Engine for Articulated Rigid Bodies with Intersection-Free Frictional Contact</a></b><br><br> Gang Yang, Siyuan Luo, Yunhai Feng, Zhixin Sun, Chenrui Tie, and Lin Shao<br><br>
    IEEE International Conference on Robotics and Automation (ICRA) 2024 <br><br>
<a href="https://arxiv.org/abs/2309.04710">paper</a>
&nbsp;<a href="https://sites.google.com/view/diffsim/">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/gamma.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://linsats.github.io">GAMMA: Generalizable Articulation Modeling and Manipulation for Articulated Objects</a></b><br><br> Qiaojun Yu, Junbo Wang, Wenhai Liu, Ce Hao, Liu Liu, Lin Shao, Weiming Wang and Cewu Lu<br><br>
    IEEE International Conference on Robotics and Automation (ICRA) 2024  <br><br>
<a href="https://linsats.github.io">paper</a>
&nbsp;<a href="https://linsats.github.io/">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>



<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/difflfd.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/diff-lfd/">Diff-LfD: Contact-aware Model-based Learning from Visual
Demonstration for Robotic Manipulation via Differentiable Physics-based
Simulation and Rendering</a></b><br><br> Xinghao Zhu, Jinghan Ke, Zhixuan Xu, Zhixin Sun, Bizhe Bai, Jun Lv, Qingtao Liu, Yuwei Zeng, Qi Ye, Cewu Lu, Masayoshi Tomizuka, Lin Shao<br><br>Conference on Robot Learning (CoRL) 2023  <b style="color: red; ">(Oral)<br><br>
<a href="https://linsats.github.io/">paper</a>
&nbsp;<a href="https://sites.google.com/view/diff-lfd">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/clothesnet.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/clothesnet/">ClothesNet: An Information-Rich 3D Garment Model Repository with Simulated Clothes Environment</a></b><br><br> Bingyang Zhou, Haoyu Zhou, Tianhai Liang, Qiaojun Yu, Siheng Zhao, Yuwei Zeng, Jun Lv, Siyuan Luo, Qiancai Wang, Xinyuan Yu, Haonan Chen, Cewu Lu, and Lin Shao<br><br>IEEE/CVF International Conference on Computer Vision (ICCV) 2023 <br><br>
<a href="https://arxiv.org/pdf/2308.09987.pdf/">paper</a>
&nbsp;<a href="https://sites.google.com/view/clothesnet/">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/dexRepGrasp.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/2303.09806.pdf">DexRepNet: Learning Dexterous Robotic Grasping Network with
Geometric and Spatial Hand-Object Representations</a></b><br><br> Qingtao Liu*, Yu Cui*, Qi Ye, Zhengnan Sun, Haoming Li, Gaofeng Li, Lin Shao, Jiming Chen<br><br>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2023 <br><br>
<a href="https://arxiv.org/pdf/2303.09806.pdf">paper</a>
&nbsp;project
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/cloth_teaser.png" width="150" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/diffsimcloth/">DiffClothAI: Differentiable Cloth Simulation with Intersection-free
Frictional Contact and Differentiable Two-Way Coupling with Articulated Rigid Bodies</a></b><br><br> Xinyuan Yu*, Siheng Zhao*, Siyuan Luo, Gang Yang, and Lin Shao<br><br>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2023 <br><br>
<a href="https://linsats.github.io/">paper</a>
&nbsp;<a href="https://sites.google.com/view/diffsimcloth/">project</a>
&nbsp;<a href="https://linsats.github.io/">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./files/samrl.png" width="180" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/sam-rl">SAM-RL: Sensing-Aware Model-based Reinforcement Learning via Differentiable Physics-based Simulation and Rendering</a></b><br><br>Jun Lv, Yunhai Feng, Cheng Zhang, Shuang Zhao, Lin Shao* and Cewu Lu*<br><br>Proceedings of Robotics: Science and Systems (RSS) 2023

   <br><br> The International Journal of Robotics Research (IJRR)<br><br>
      <b style="color: red; ">Best System Paper Award finalist </b>  <br><br>
<a href="https://arxiv.org/abs/2210.15185">paper</a>
&nbsp;<a href="https://sites.google.com/view/sam-rl">project</a>
&nbsp;<a href="https://sites.google.com/view/sam-rl">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/sgci.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/egci">SAGCI-System: Towards Sample-Efficient, Generalizable, Compositional, and Incremental Robot Learning</a></b><br><br>Jun Lv*, Qiaojun Yu*, Lin Shao*, Wenhai Liu, Wenqiang Xu and Cewu Lu<br><br>IEEE International Conference on Robotics and Automation (ICRA) 2022 <br><br>
<a href="https://arxiv.org/abs/2111.14693">paper</a>
&nbsp;<a href="https://sites.google.com/view/egci">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=V3rcTVBktec">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/roboAssembly.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/roboticassembly">RoboAssembly:  Learning  Generalizable  Furniture  Assembly  Policy in  a  Novel  Multi-robot  Contact-rich  Simulation  Environment</a></b><br><br>Mingxin Yu*, Lin Shao*, Zhehuan Chen, Tianhao Wu, Qingnan Fan, Kaichun Mo, and Hao Dong<br><br>Arxiv Preprint, 2021 <br><br>
<a href="https://arxiv.org/abs/2112.10143">paper</a>
&nbsp;<a href="https://sites.google.com/view/roboticassembly">project</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/regrasp.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/regrasp">Learning to Regrasp by Learning to Place</a></b><br><br>Shuo Cheng, Kaichun Mo, Lin Shao<br><br>Conference on Robot Learning (CoRL) 2021<br><br>
<a href="https://arxiv.org/pdf/2109.08817.pdf">paper</a>
&nbsp;<a href="https://sites.google.com/view/regrasp">project</a>
&nbsp;<a href="https://github.com/touristCheng/Learning2Regrasp">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>



<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/DTAC.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/gracdrl">GRAC: Self-Guided and Self-Regularized Actor-Critic</a></b><br><br>Lin Shao, Yifan You, Mengyuan Yan, Shenli Yuan, Qingyun Sun, Jeannette Bohg<br><br>Conference on Robot Learning (CoRL) 2021<br><br>
<a href="https://arxiv.org/pdf/2009.08973.pdf">paper</a>
&nbsp;<a href="https://sites.google.com/view/gracdrl">project</a>
&nbsp;<a href="https://github.com/stanford-iprl-lab/GRAC">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/hangv2.png" width="170" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/hangingobject">OmniHang: Learning to Hang Arbitrary Objects Using Contact Point Correspondences and Neural Collision Estimation </a></b><br><br>Yifan You*, Lin Shao*, Toki Migimatsu, Jeannette Bohg<br><br>IEEE International Conference on Robotics and Automation (ICRA) 2021<br><br>
<a href="https://arxiv.org/abs/2103.14283">paper</a>
&nbsp;<a href="https://sites.google.com/view/hangingobject">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=yurtzk6Jy7o&ab_channel=LinShao">video</a> 
&nbsp;<a href="https://github.com/yifan-you-37/omnihang">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>




<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/partAssembly2.png" width="170" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/abs/2006.07793">	
Generative 3D Part Assembly via Dynamic Graph Learning</a></b><br><br>Jialei Huang*, Guanqi Zhan*, Qingnan Fan, Kaichun Mo, Lin Shao, Baoquan Chen, Leonidas J. Guibas, Hao Dong<br><br>NeurIPS 2020<br><br>
<a href="https://arxiv.org/abs/2006.07793">paper</a>
&nbsp;<a href="https://hyperplane-lab.github.io/Generative-3D-Part-Assembly/">project</a>
&nbsp;<a href="https://github.com/Championchess/Generative-3D-Part-Assembly">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/conceptlearning.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://sites.google.com/view/concept2robot">Concept2Robot: Learning Manipulation Concepts from Instructions and Human Demonstrations</a></b><br><br>Lin Shao, Toki Migimatsu, Qiang Zhang,  Karen Yang, Jeannette Bohg<br><br>Proceedings of Robotics: Science and Systems (RSS) 2020<br><br>
    The International Journal of Robotics Research (IJRR)<br><br>
<a href="http://www.roboticsproceedings.org/rss16/p082.pdf">paper</a>
&nbsp;<a href="https://sites.google.com/view/concept2robot">project</a>
&nbsp;<a href="https://sites.google.com/view/concept2robot">video</a>
&nbsp;<a href="https://www.youtube.com/watch?v=flxrirLbxzg">presentation</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>


<!--Part Assembly-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/partAssembly.png" width="225" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/abs/2003.09754">Learning 3D Part Assembly from a Single Image</a></b><br><br>Yichen Li*, Kaichun Mo*, Lin Shao, Minhyuk Sung, Leonidas J. Guibas<br><br>ECCV 2020<br><br>
<a href="https://arxiv.org/abs/2003.09754">paper</a>
&nbsp;<a href="https://arxiv.org/abs/2003.09754">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=gtaBaEAs22s">presentation</a>
&nbsp;<a href="https://github.com/AntheaLi/3DPartAssembly">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!-- Roller Grasper-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/rollerGrasper.png" height="145" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/2004.08499.pdf">Design and Control of Roller Grasper V2 for In-Hand Manipulation</a></b><br><br>Shenli Yuan, Lin Shao, Connor L. Yako, Alex Gruebele, and J. Kenneth Salisbury<br><br>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2020<br><br>
<a href="https://arxiv.org/pdf/2004.08499.pdf">paper</a>
&nbsp;<a href="https://ccrma.stanford.edu/~shenliy/roller_grasper_v2.html">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=r_HaJfANyT8">video</a>
&nbsp;<a href="https://github.com/yuanshenli/RollerGrasperV2">code</a>
&nbsp;<a href="https://spectrum.ieee.org/automaton/robotics/robotics-hardware/we-can-do-better-than-humanlike-hands-for-robots">Featured in IEEE Spectrum</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!-- Scaffolding-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/scaffold.png" height="140" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/1911.00969.pdf">Learning to Scaffold the Development of Robotic Manipulation Skills</a></b><br><br>Lin Shao, Toki Migimatsu, Jeannette Bohg<br><br>IEEE International Conference on Robotics and Automation (ICRA) 2020<br><br>
<a href="https://arxiv.org/pdf/1911.00969.pdf">paper</a>
&nbsp;<a href="https://sites.google.com/view/scaffoldlearning">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=od3jBAJES4w">video</a>
&nbsp;<a href="https://github.com/stanford-iprl-lab/ScaffoldLearning">code</a>
&nbsp;<a href="https://www.youtube.com/watch?v=w-O6dxdVMpY&feature=youtu.be&ab_channel=LinShao">presentation</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!-- UniGrasp-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/metagrasp.png" height="140" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/1910.10900.pdf">UniGrasp: Learning a Unified Model to Grasp with Multifingered Robotic Hands</a></b><br><br>Lin Shao,  Fabio Ferreira*, Mikael Jorda*, Varun Nambiar*, Jianlan Luo, Eugen Solowjow, Juan Aparicio Ojea, Oussama Khatib, Jeannette Bohg<br><br>IEEE Robotics and Automation Letters with ICRA 2020 option<br><br>
<a href="https://arxiv.org/pdf/1910.10900.pdf">paper</a>
&nbsp;<a href="https://sites.google.com/view/unigrasp">project</a>
&nbsp;<a href="https://www.youtube.com/watch?v=UqVXL9QDnPU">video</a>
&nbsp;<a href="https://github.com/stanford-iprl-lab/UniGrasp">code</a>
&nbsp;<a href="https://www.youtube.com/watch?v=wv9h1ADJfDE&ab_channel=LinShao">presentation</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

</tbody>


<!-- Scene Flow -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/sceneflownet.png" height="140" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/1804.05195.pdf">Motion-based Object Segmentation 
based on Dense RGB-D Scene Flow</a></b><br><br>Lin Shao, Parth Shah<sup>*</sup>, Vikranth Dwaracherla<sup>*</sup>, Jeannette Bohg<br><br>IEEE Robotics and Automation Letters with IROS 2018 option<br><br>
<a href="https://arxiv.org/pdf/1804.05195.pdf">paper</a>
&nbsp;<a href="https://stanford-iprl-lab.github.io/sceneflownet/">project</a>
&nbsp;<a href="https://youtu.be/adAMsLraq9o">video</a>
&nbsp;<a href="https://stanford-iprl-lab.github.io/sceneflownet/">code</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

</tbody>

<!-- Size Transferring -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./files/attributeTransfer.png" height="140" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://ieeexplore.ieee.org/abstract/document/8374619/">Cross-modal Attribute Transfer for Rescaling 3D Models</a></b><br><br>Lin Shao, Angel X. Chang, Hao Su, Manolis Savva, Leonidas J. Guibas<br><br>3DV 2017<br><br>
<a href="https://ieeexplore.ieee.org/abstract/document/8374619/">paper</a>
&nbsp;<a href="https://linsats.github.io/attributeTransferring/">project</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

</tbody>
</table></ul></div>



<a name="teaching"></a>
<h3>Teaching Experiences</h3>
<div class="mainsection">
<ul>
<li>
Teaching Assistant. Spring 2020-21: <a href="http://cs231n.stanford.edu/">Convolutional Neural Networks for Visual Recognition (CS231N)</a>
</li>
</ul>
<ul>
<li>
Teaching Assistant. Winter 2017-18: <a href="https://cs.stanford.edu/groups/manips/teaching/cs223a/">Introduction to Robotics (CS223A)</a>
</li>
</ul>
<ul>
<li>
Teaching Assistant. Spring 2016-17: <a href="http://graphics.stanford.edu/courses/cs468-17-spring/">Machine Learning for 3D Data (CS468)</a>
</li>
</ul>
</div>


<h3>Professional Activities</h3>
<div class="mainsection">
<ul>
<li>
Conference Reviewer: 
<ul><li>IEEE International Conference on Robotics and Automation (ICRA), 2020/19/18</li></ul> 
<ul><li>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020/19/18</li></ul> 
<ul><li>Proceedings of Robotics: Science and Systems (RSS), 2019</li></ul>  
<ul><li>International Symposium on Robotics Research (ISRR), 2019</li></ul> 
<ul><li>Conference on Robot Learning (CoRL)</li></ul> 
<ul><li>International Conference on Machine Learning (ICML)</li></ul> 
<ul><li>International Conference on Learning Representations (ICLR)</li></ul> 
</li>
</ul>
<ul>
<li>
Journal Reviewer:
<ul><li>IEEE Robotics and Automation Letters (RA-L) 2020/19/18</li></ul>
<ul><li>IEEE Transactions on Robotics (T-RO)</li></ul>    
<ul><li>IEEE Transactions on Cognitive and Developmental Systems (TCDS)</li></ul>   
</li>
</ul>
</div>

</body><div></div></html>
